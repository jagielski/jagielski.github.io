<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>TPDP 2024 Talk Notes</title>
</head>
<body>
  <div class="container">
    <h1>TPDP 2024 Talk Notes</h1>

    This page provides notes for my <a href="https://tpdp.journalprivacyconfidentiality.org/2024/">TPDP 2024</a> talk called "Data and Privacy in Data Privacy".
    
    <h2>Training Data Attribution</h2>
    <h3>Short Summary</h3>
    Differential privacy, membership inference, and training data attribution are all research areas that care a lot about "counterfactual worlds" where certain examples are included or excluded. Right now, there is not much overlap between these areas (both in the people working on them and the techniques), and my claim here is that there should be.

    <h3>Reading List</h3>
    <ul>
      <li>
        <a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail.</a> This paper is one of my all time favorites and it's worth incorporating into your intuition. It led directly to Feldman and Zhang's paper <a href="https://arxiv.org/abs/2008.03703">What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation.</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles.</a> This paper takes inspiration from the Feldman and Zhang paper and proposes an at-the-time state-of-the-art membership inference attack. It's not the first paper using the Feldman and Zhang approach in membership inference, read the paper for more details.
      </li>
      <li>
        <a href="https://arxiv.org/abs/2202.00622">Datamodels: Predicting Predictions from Training Data</a>. A very cool paper which improves upon the Feldman and Zhang influence estimation approach by treating it as a prediction problem. Their initial experiments use exactly the models that Feldman and Zhang trained!!
      </li>
      <li>
        <a href="https://arxiv.org/abs/2303.14186">TRAK: Attributing Model Behavior at Scale</a>. A more efficient method for training data attribution. More recently, there's also <a href="https://arxiv.org/abs/2405.13954">LoGra</a> and <a href="https://arxiv.org/abs/2308.03296">recent work</a> from Anthropic on <a href="https://arxiv.org/abs/1703.04730">influence functions</a>. For even more resources, see the <a href="https://ml-data-tutorial.org/">ICML attribution tutorial</a>.
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Can attribution be used to better characterize privacy leakage in private prediction? Starting points: <a href="https://arxiv.org/abs/2306.07381">"Private Prediction Strikes Back!"</a>, <a href="https://arxiv.org/abs/2402.09403">Auditing Private Prediction</a></li>      
      <li>Can algorithms or theoretical tools from differential privacy be used to better understand attribution, either theoretically or empirically?</li>
      <li>Do any properties of membership inference also hold for training data attribution (or vice versa)? Examples: <a href="https://arxiv.org/abs/2206.10469">Privacy Onion Effect</a>, <a href="https://arxiv.org/abs/2207.00099">Forgetting</a></li>
    </ul>
    
    <h2>Data Curation</h2>
    <h3>Short Summary</h3>
    Curating data has become very important to training state of the art models. However, there is limited investigation of the implications and opportunities of data curation for privacy.
    
    <h3>Reading List</h3>
    <ul>
      <li>
        Some non-private pretraining data selection papers: <a href="https://arxiv.org/abs/2304.14108">DataComp (CLIP)</a> and <a href="https://arxiv.org/abs/2406.11794">DataComp (LLM)</a>, <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">Llama 3</a>. DataComps are competitions to produce the best pretraining data filtering recipe. Llama 3 is of course a model, but they describe (on a high level) how they did their training data preprocessing.
      </li>
      <li>
        Selecting pretraining data for private finetuning papers: <a href="https://arxiv.org/abs/2303.01256">Gradient Subspace Distance</a>, <a href="https://arxiv.org/abs/2305.13865">Selective Pre-training for Private Fine-tuning</a>, <a href="">Prompt LLMs to Synthesize Data for Private Applications</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2309.05610">Privacy Side Channels in Machine Learning Systems</a>. Data curation such as deduplication on private training data can lead to adaptive privacy attacks!
      </li>
      <li>
        <a href="https://arxiv.org/abs/2403.13041">Provable Privacy with Non-Private Pre-Processing</a>. Shows how to account for privacy leakage from nonprivate data processing such as deduplication.
      </li>
      <li>
        The recent ICML best paper <a href="https://arxiv.org/abs/2212.06470">Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining</a> has arguments which relate to topics in this section. It's worth reading and might lead to some other questions.
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Does curating private data help differentially private training? Or does having more data always help?</li>
      <li>What is the best way to do pretraining data selection for a private downstream task?</li>
      <li>Are there other data curation algorithms with negative privacy/security implications?</li>
    </ul>
    
    <h2>Privacy Semantics</h2>
    <h3>Short Summary</h3>
    The ML privacy literature has begun to consider different "privacy semantics". Often dealing more with access control-type approaches rather than differential privacy, the DP community's experience thinking about privacy may be helpful here.
    
    <h3>Reading List</h3>
    <ul>
      <li>
        Contextual integrity in LLMs: <a href="https://arxiv.org/abs/2310.17884">Can LLMs keep a Secret?</a>, <a href="https://arxiv.org/abs/2408.02373">Contextual Integrity in Privacy-Conscious Assistants</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2401.15774">Contextual integrity + DP</a>
      </li>
      <li>
        Retrieval-augmented LLMs (with applications to machine unlearning): <a href="https://arxiv.org/abs/2308.04430">SILO Language Models</a>. The relevant research direction here is called retrieval-augmented generation (RAG). The backbone retrieval-augmentation technique for SILO is <a href="https://arxiv.org/abs/1911.00172">kNN-LM</a>. Some other RAG flavors include <a href="https://arxiv.org/abs/2112.04426">RETRO</a> and in context RAG (e.g. <a href="https://arxiv.org/abs/2210.09150">a</a>, <a href="https://arxiv.org/abs/2212.14024">b</a>, <a href="https://arxiv.org/abs/2210.09150">c</a>). Most discussion of RAG these days refers to in context RAG techniques of some form. Language models for search such as Google Search Generative Experience, the Bing Chatbot, or Perplexity can be seen as a form of in context RAG, where the "retriever" is the search engine itself!
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Are there applications/threat models where combining some of these different privacy semantics (including DP) makes sense?</li>
      <li>Are there interesting attacks on these systems?</li>
    </ul>
  </div>
</body>
</html>
