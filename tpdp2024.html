<!doctype html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>TPDP 2024 Talk Notes</title>
</head>
<body>
  <div class="container">
    <h1>TPDP 2024 Talk Notes</h1>

    This page provides notes for my <a href="https://tpdp.journalprivacyconfidentiality.org/2024/">TPDP 2024</a> talk called "Data and Privacy in Data Privacy".
    
    <h2>Training Data Attribution</h2>
    
    <h3>Reading List</h3>
    <ul>
      <li>
        <a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail.</a> This paper is one of my all time favorites and it's worth incorporating into your intuition. It led directly to Feldman and Zhang's paper <a href="https://arxiv.org/abs/2008.03703">What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation.</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2112.03570">Membership Inference Attacks From First Principles.</a> This paper takes inspiration from the Feldman and Zhang paper and proposes an at-the-time state-of-the-art membership inference attack. It's not the first paper using the Feldman and Zhang approach in membership inference, read the paper for more details.
      </li>
      <li>
        <a href="https://arxiv.org/abs/2202.00622">Datamodels: Predicting Predictions from Training Data</a>. A very cool paper which improves upon the Feldman and Zhang influence estimation approach by treating it as a prediction problem. Their initial experiments use exactly the models that Feldman and Zhang trained!!
      </li>
      <li>
        <a href="https://arxiv.org/abs/2303.14186">TRAK: TRAK: Attributing Model Behavior at Scale</a>. A more efficient method for training data attribution. More recently, there's also <a href="https://arxiv.org/abs/2405.13954">LoGra</a> and <a href="https://arxiv.org/abs/2308.03296">recent work</a> from Anthropic on <a href="https://arxiv.org/abs/1703.04730">influence functions</a>.
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Can attribution be used to better characterize privacy leakage in private prediction? Starting points: <a href="https://arxiv.org/abs/2306.07381">"Private Prediction Strikes Back!"</a>, <a href="https://arxiv.org/abs/2402.09403">Auditing Private Prediction</a></li>      
      <li>Can algorithms or theoretical tools from differential privacy be used to better understand attribution, either theoretically or empirically?</li>
      <li>Do any properties of membership inference also hold for training data attribution (or vice versa)? Examples: <a href="https://arxiv.org/abs/2206.10469">Privacy Onion Effect</a>, <a href="https://arxiv.org/abs/2207.00099">Forgetting</a></li>
    </ul>
    
    <h2>Data Curation</h2>
    
    <h3>Reading List</h3>
    <ul>
      <li>
        Some non-private pretraining data selection papers: <a href="https://arxiv.org/abs/2304.14108">DataComp (CLIP)</a> and <a href="https://arxiv.org/abs/2406.11794">DataComp (LLM)</a>, <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">Llama 3</a>. DataComps are competitions to produce the best pretraining data filtering recipe. Llama 3 is of course a model, but they describe (on a high level) how they did their training data preprocessing.
      </li>
      <li>
        Selecting pretraining data for private finetuning papers: <a href="https://arxiv.org/abs/2303.01256">Gradient Subspace Distance</a>, <a href="https://arxiv.org/abs/2305.13865">Selective Pre-training for Private Fine-tuning</a>, <a href="">Prompt LLMs to Synthesize Data for Private Applications</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/2309.05610">Privacy Side Channels in Machine Learning Systems</a>. Data curation such as deduplication on private training data can lead to adaptive privacy attacks!
      </li>
      <li>
        <a href="https://arxiv.org/abs/2403.13041">Provable Privacy with Non-Private Pre-Processing</a>. Shows how to account for privacy leakage from nonprivate data processing such as deduplication.
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Does curating private data help differentially private training? Or does having more data always help?</li>
      <li>What is the best way to do pretraining data selection for a private downstream task?</li>
      <li>Are there other data curation algorithms with negative privacy/security implications?</li>
    </ul>
    
    <h2>Privacy Semantics</h2>
    
    <h3>Reading List</h3>
    <ul>
      <li>
        <!-- Example link; replace with actual paper link if available -->
        <a href="#">Privacy Semantics Paper 1</a>
      </li>
      <li>
        <!-- Example link; replace with actual paper link if available -->
        <a href="#">Privacy Semantics Paper 2</a>
      </li>
    </ul>

    <h3>Open Questions/Directions</h3>
    <ul>
      <li>Open Question 1</li>
      <li>Open Question 2</li>
    </ul>
  </div>
</body>
</html>
