<!doctype html>
<html lang="en-US">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <link href="https://fonts.googleapis.com/css?family=Lora|Roboto:500" rel="stylesheet">
  <link href="styles.css" rel="stylesheet">

  <title>Matthew Jagielski</title>
</head>
<body>
  <div class="container mt-3">
    <div class="row">
      <div id="About Me" class="col-6">
      	<h1>Matthew Jagielski</h1>
      	Google Research
      	<br>
      	Email: (my last name)@google.com
      	<br>
        <a href="https://github.com/jagielski">Github</a>
        <br>
	<a href="https://scholar.google.com/citations?user=_8rw_GMAAAAJ&hl=en">Google Scholar</a>
      </div>
      <div class="col-5 d-none d-sm-block">
        <img src="images/me.jpg" alt="me.jpg" height="300px" width="auto">
      </div>
    </div>
    <div class="row mt-3">
      <div class="col">
        <h6>News</h6>
	You should submit to the <a href="https://dependablesecureml.github.io/cfp.html">DSML 2023 workshop</a>, colocated with DSN 2023 in Porto, Portugal! The deadline is February 28, and the workshop will be on June 27. We're soliciting submissions on lots of cool areas in the intersection of security, dependability, and machine learning. I'm cochairing with <a href="https://lishanyang.github.io/">Lishan Yang</a>.
	<br>
	<br>
	<h6>About Me</h6>
	I am a research scientist at Google Brain, working on <a href="https://www.cs.jhu.edu/~terzis/">Andreas Terzis's team</a>. My research is broadly at the intersection of machine learning, security, and privacy. I like to investigate and improve the level of security and privacy offered by machine learning algorithms against real world adversaries.
	<br>
	<br>
	I did my PhD with <a href="http://www.ccs.neu.edu/home/alina/">Alina Oprea</a> and <a href="https://cnitarot.github.io/">Cristina Nita-Rotaru</a>, as a member of the Network and Distributed Systems Security Lab (<a href="https://nds2.ccs.neu.edu/">NDS2</a>). They have a great group - you should go work with them! I have also done two internships at Google, working with <a href="https://www.papernot.fr/">Nicolas Papernot</a> on model extraction attacks, and with the Google DoS and Abuse team, using machine learning to protect Google Cloud customers from DoS attacks.
	<br>
        <br>
	In other news, I enjoy running, swimming, and biking. I'm also a retired <a href="https://www.youtube.com/watch?v=3lHG5ywn0P8">Super Smash Brothers tournament competitor</a>.
      </div>
    </div>
    <br>
    <div class="row mt-3">
      <div class="col">
        <h6>Selected Publications - see <a href="https://scholar.google.com/citations?user=_8rw_GMAAAAJ&hl=en">Google Scholar</a> for full list</h6>
        <ul>
		  <li>
			Measuring Forgetting of Memorized Training Examples
			<br>
			Matthew Jagielski, Om Thakkar, Florian Tram√®r, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Chiyuan Zhang
			<br>
			ICLR 2023
			<br>
			<a href="https://arxiv.org/abs/2207.00099">[Paper]</a>
		  </li>
		  <li>
			Extracting Training Data from Large Language Models
			<br>
			Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel
			<br>
			USENIX Security 2021
			<br>
			<a href="https://arxiv.org/abs/2012.07805">[Paper]</a>
		  </li>
		  <li>
			Auditing Differentially Private Machine Learning - How Private is Private SGD?
			<br>
			Matthew Jagielski, Jonathan Ullman, Alina Oprea
			<br>
			NeurIPS 2020, TPDP 2020 Contributed Talk
			<br>
			<a href="https://arxiv.org/abs/2006.07709">[Paper]</a> <a href="https://github.com/jagielski/auditing-dpsgd">[Code]</a> <a href="files/neuripsposter.pdf">[Poster]</a> <a href="https://youtu.be/3Qp4GVXfMPE">[3min talk]</a>
		  </li>
		  <li>
			High-Fidelity Extraction of Neural Network Models
			<br>
			Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot
			<br>
			USENIX Security 2020
			<br>
			<a href="https://arxiv.org/abs/1909.01838">[Paper]</a> <a href="http://www.cleverhans.io/2020/05/21/model-extraction.html">[Blog]</a> <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski">[Talk]</a>
		  </li>
          	<li>
            		Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning
            		<br>
            		Matthew Jagielski, Alina Oprea, Chang Liu, Cristina Nita-Rotaru, and Bo Li
            		<br>
            		IEEE S&P (Oakland) 2018
            		<br>
            		<a href="https://github.com/jagielski/manip-ml">[Code]</a> <a href="https://arxiv.org/abs/1804.00308">[Paper]</a> <a href="https://www.youtube.com/watch?v=ahC4KPd9lSY">[Talk]</a>
          	</li>
        </ul>
      </div>
    </div>
  </div>
</body>
</html>

